{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7lu6wcxC0etiAiwxKtXAn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emiliawisnios/Social-and-Public-Policy-python/blob/main/Notebooks/Social_and_Public_Policy_Coding_Python_07_14_11_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today's class we will focus on text documents processing.\n",
        "\n",
        "Next time we will work on data scraping."
      ],
      "metadata": {
        "id": "v4rZr3_VHQb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment setup"
      ],
      "metadata": {
        "id": "7iaYnZ3rJK6D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJAib5HIHLXw",
        "outputId": "8899f1bb-95aa-44ae-bf42-9c21fc4c1fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# Required libraries for text processing\n",
        "# !pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Handling"
      ],
      "metadata": {
        "id": "BbpVqCljJQpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Text Files\n",
        "\n",
        "In political science research, we often work with various text documents like:\n",
        "- Policy documents\n",
        "- Speech transcripts\n",
        "- Legislative texts\n",
        "- Social media data\n",
        "\n",
        "\n",
        "Let's learn how to handle these files in Python."
      ],
      "metadata": {
        "id": "-y-2IuTUJkc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Basic file reading\n",
        "def read_simple_file(filename):\n",
        "    \"\"\"\n",
        "    Basic function to read a text file\n",
        "    \"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        return file.read()"
      ],
      "metadata": {
        "id": "6u8Q4DKyJP_M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE 1:\n",
        "\n",
        "Create a text file named `speech.txt` with any political speech. Try reading it using the function above.\n"
      ],
      "metadata": {
        "id": "f0uOxb8jJqvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "read_simple_file('/content/speech.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "fH7jq8m0H8YT",
        "outputId": "d18592f3-32e2-4a8d-9489-50829909791b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Księga pierwsza\\n\\n\\n\\nGospodarstwo\\n\\nPowrót panicza — Spotkanie się pierwsze w pokoiku, drugie u stołu — Ważna Sędziego nauka o grzeczności — Podkomorzego uwagi polityczne nad modami — Początek sporu o Kusego i Sokoła — Żale Wojskiego — Ostatni Woźny Trybunału — Rzut oka na ówczesny stan polityczny Litwy i Europy\\n\\n    Litwo! Ojczyzno moja! ty jesteś jak zdrowie:\\nIle cię trzeba cenić, ten tylko się dowie,\\nKto cię stracił. Dziś piękność twą w całej ozdobie\\nWidzę i opisuję, bo tęsknię po tobie.\\n\\n    Panno święta, co Jasnej bronisz Częstochowy\\nI w Ostrej świecisz Bramie! Ty, co gród zamkowy\\nNowogródzki ochraniasz z jego wiernym ludem!\\nJak mnie dziecko do zdrowia powróciłaś cudem\\n(Gdy od płaczącej matki, pod Twoją opiekę\\nOfiarowany, martwą podniosłem powiekę;\\nI zaraz mogłem pieszo, do Twych świątyń progu\\nIść za wrócone życie podziękować Bogu),\\nTak nas powrócisz cudem na Ojczyzny łono.\\nTymczasem przenoś moją duszę utęsknioną\\nDo tych pagórków leśnych, do tych łąk zielonych,\\nSzeroko nad błękitnym Niemnem rozciągnionych;\\nDo tych pól malowanych zbożem rozmaitem,\\nWyzłacanych pszenicą, posrebrzanych żytem;\\nGdzie bursztynowy świerzop, gryka jak śnieg biała,\\nGdzie panieńskim rumieńcem dzięcielina pała,\\nA wszystko przepasane jakby wstęgą, miedzą\\nZieloną, na niej z rzadka ciche grusze siedzą.\\n\\n    Śród takich pól przed laty, nad brzegiem ruczaju,\\nNa pagórku niewielkim, we brzozowym gaju,\\nStał dwór szlachecki, z drzewa, lecz podmurowany;\\nŚwieciły się z daleka pobielane ściany,\\nTym bielsze, że odbite od ciemnej zieleni\\nTopoli, co go bronią od wiatrów jesieni.\\nDom mieszkalny niewielki, lecz zewsząd chędogi,\\nI stodołę miał wielką, i przy niej trzy stogi\\nUżątku, co pod strzechą zmieścić się nie może.\\nWidać, że okolica obfita we zboże,\\nI widać z liczby kopic, co wzdłuż i wszerz smugów\\nŚwiecą gęsto jak gwiazdy, widać z liczby pługów\\nOrzących wcześnie łany ogromne ugoru,\\nCzarnoziemne, zapewne należne do dworu,\\nUprawne dobrze na kształt ogrodowych grządek:\\nŻe w tym domu dostatek mieszka i porządek.\\nBrama na wciąż otwarta przechodniom ogłasza,\\nŻe gościnna, i wszystkich w gościnę zaprasza.\\n\\n    Właśnie dwukonną bryką wjechał młody panek\\nI obiegłszy dziedziniec zawrócił przed ganek.\\nWysiadł z powozu; konie porzucone same,\\nSzczypiąc trawę ciągnęły powoli pod bramę.\\nWe dworze pusto: bo drzwi od ganku zamknięto\\nZaszczepkami i kołkiem zaszczepki przetknięto.\\nPodróżny do folwarku nie biegł sług zapytać,\\nOdemknął, wbiegł do domu, pragnął go powitać.\\nDawno domu nie widział, bo w dalekim mieście\\nKończył nauki, końca doczekał nareszcie.\\nWbiega i okiem chciwie ściany starodawne\\nOgląda czule, jako swe znajome dawne.\\nTeż same widzi sprzęty, też same obicia,\\nZ którymi się zabawiać lubił od powicia,\\nLecz mniej wielkie, mniej piękne niż się dawniej zdały.\\nI też same portrety na ścianach wisiały:\\nTu Kościuszko w czamarce krakowskiej, z oczyma\\nPodniesionymi w niebo, miecz oburącz trzyma;\\nTakim był, gdy przysięgał na stopniach ołtarzów,\\nŻe tym mieczem wypędzi z Polski trzech mocarzów,\\nAlbo sam na nim padnie. Dalej w polskiej szacie\\nSiedzi Rejtan, żałośny po wolności stracie;\\nW ręku trzyma nóż ostrzem zwrócony do łona,\\nA przed nim leży Fedon i żywot Katona.\\nDalej Jasiński, młodzian piękny i posępny;\\nObok Korsak, towarzysz jego nieodstępny:\\nStoją na szańcach Pragi, na stosach Moskali,\\nSiekąc wrogów, a Praga już się wkoło pali.\\nNawet stary stojący zegar kurantowy\\nW drewnianej szafie poznał, u wniścia alkowy;\\nI z dziecinną radością pociągnął za sznurek,\\nBy stary Dąbrowskiego usłyszeć mazurek.\\n\\n    Biegał po całym domu i szukał komnaty,\\nGdzie mieszkał dzieckiem będąc, przed dziesięciu laty.\\nWchodzi, cofnął się, toczył zdumione źrenice\\nPo ścianach: w tej komnacie mieszkanie kobiéce!\\nKtóż by tu mieszkał? Stary stryj nie był żonaty;\\nA ciotka w Petersburgu mieszkała przed laty.\\nTo nie był ochmistrzyni pokój? Fortepiano?\\nNa nim nuty i książki; wszystko porzucano\\nNiedbale i bezładnie: nieporządek miły!\\nNiestare były rączki, co je tak rzuciły.\\nTuż i sukienka biała, świeżo z kołka zdjęta\\nDo ubrania, na krzesła poręczu rozpięta;\\nA na oknach donice z pachnącymi ziołki,\\nGeranium, lewkonija, astry i fijołki.\\nPodróżny stanął w jednym z okien — nowe dziwo:\\nW sadzie, na brzegu niegdyś zarosłym pokrzywą,\\nBył maleńki ogródek ścieżkami porznięty,\\nPełen bukietów trawy angielskiej i mięty.\\nDrewniany, drobny, w cyfrę powiązany płotek\\nPołyskał się wstążkami jaskrawych stokrotek;\\nGrządki, widać, że były świeżo polewane,\\nTuż stało wody pełne naczynie blaszane,\\nAle nigdzie nie widać było ogrodniczki;\\nTylko co wyszła: jeszcze kołyszą się drzwiczki\\nŚwieżo trącone, blisko drzwi ślad widać nóżki\\nNa piasku, bez trzewika była i pończoszki;\\nNa piasku drobnym, suchym, białym na kształt śniegu,\\nŚlad wyraźny, lecz lekki, odgadniesz, że w biegu\\nChybkim był zostawiony nóżkami drobnemi\\nOd kogoś, co zaledwie dotykał się ziemi.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different ways to read files:\n",
        "1. `read()` - entire file as a single string\n",
        "2. `readlines()` - list of lines\n",
        "3. `readline()` - one line at a time"
      ],
      "metadata": {
        "id": "liwFMeIfJ3I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Reading line by line (memory efficient for large files)\n",
        "def read_large_file(filename):\n",
        "    \"\"\"\n",
        "    Memory-efficient function to process large files line by line\n",
        "    \"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            # Process each line\n",
        "            yield line.strip()"
      ],
      "metadata": {
        "id": "oLEX6uX7KRR4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "large_file_lines = read_large_file('/content/speech.txt')\n",
        "for line in large_file_lines:\n",
        "    print(line)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFpx_JyXvh26",
        "outputId": "6d670a3c-4138-4a37-88f3-e7a5a9dab852"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Księga pierwsza\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE 2:\n",
        "\n",
        "Create a function that:\n",
        "1. Reads a file\n",
        "2. Counts the number of lines\n",
        "3. Counts the total number of words\n",
        "4. Returns both counts"
      ],
      "metadata": {
        "id": "9yIbD9iBKSwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word_tokenize for calculating number of words"
      ],
      "metadata": {
        "id": "oEzXVwlnwKUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d7IKMayzZbO",
        "outputId": "c112f03f-7c1c-45be-a269-5e715a3ca4f1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "def file_statistics(filename):\n",
        "  number_of_lines = 0\n",
        "  number_of_words = 0\n",
        "\n",
        "  ########### HERE GOES YOUR CODE ################\n",
        "  large_file_lines = read_large_file(filename)\n",
        "  for line in large_file_lines:\n",
        "      number_of_lines += 1 # number_of_lines = number_of_lines + 1\n",
        "      number_of_words += len(word_tokenize(line))\n",
        "\n",
        "  return number_of_lines, number_of_words\n",
        "\n",
        "file_statistics('/content/speech.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgnZUbg8KZcq",
        "outputId": "76721d7a-dd1b-48df-888d-c321573755e9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(114, 888)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleaning and Normalization"
      ],
      "metadata": {
        "id": "UAYMgWTKKedq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Basic text cleaning function\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Input text\n",
        "\n",
        "    Returns:\n",
        "    str: Cleaned text\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "hoo-uMaIKjao"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Working with real political text\n",
        "sample_text = \"\"\"\n",
        "The United Nations (UN) was established in 1945, after World War II.\n",
        "Its primary purpose is to maintain international peace & security.\n",
        "The UN has 193 Member States as of 2024!\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nCleaned text:\")\n",
        "print(clean_text(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KhNMCvHK1JK",
        "outputId": "7f787d23-8b3f-48e5-8c19-ffaf24abedc2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "\n",
            "The United Nations (UN) was established in 1945, after World War II. \n",
            "Its primary purpose is to maintain international peace & security.\n",
            "The UN has 193 Member States as of 2024!\n",
            "\n",
            "\n",
            "Cleaned text:\n",
            "the united nations un was established in after world war ii its primary purpose is to maintain international peace security the un has member states as of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE 3:\n",
        "\n",
        "Enhance the clean_text function to:\n",
        "1. Remove specific words (like 'the', 'and', 'or')\n",
        "2. Handle special characters\n",
        "3. Remove specific patterns (like dates, URLs)\n",
        "\n",
        "\n",
        "Write test cases for your enhanced function"
      ],
      "metadata": {
        "id": "gpmS7EI3Kfa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "def clean_text_extended(text):\n",
        "    \"\"\"\n",
        "    Basic text cleaning function\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Input text\n",
        "\n",
        "    Returns:\n",
        "    str: Cleaned text\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removal of urls\n",
        "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    text = url_pattern.sub('', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove specific words\n",
        "    words_to_remove = ['the', 'and', 'or']\n",
        "    for word in words_to_remove:\n",
        "        text = text.replace(word, '')\n",
        "\n",
        "    # Handle special characters\n",
        "    special_characters = ['&']\n",
        "    for char in special_characters:\n",
        "        text = text.replace(char, '')\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "DmWOUplFK95R"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = 'THIS is a class of the the the or and & http://www.google.com python.'\n",
        "clean_text_extended(test1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XMxiSpkP35B8",
        "outputId": "3a23105e-8cd7-4544-e3d2-edd548e0360b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is a class of python'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing and Information Extraction"
      ],
      "metadata": {
        "id": "fLwpqUQULaW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(text):\n",
        "    \"\"\"\n",
        "    Extract basic entities from text\n",
        "    \"\"\"\n",
        "    # Tokenize into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Tokenize into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    return {\n",
        "        'sentence_count': len(sentences),\n",
        "        'word_count': len(words),\n",
        "        'filtered_word_count': len(filtered_words),\n",
        "        'unique_words': len(set(filtered_words))\n",
        "    }\n"
      ],
      "metadata": {
        "id": "NbPWEWzkLp1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 4: Analyzing political text\n",
        "policy_text = \"\"\"\n",
        "The Green New Deal is a proposed package of United States legislation\n",
        "that aims to address climate change and economic inequality.\n",
        "The proposal calls for meeting 100% of the power demand in the United States\n",
        "through clean, renewable, and zero-emission energy sources.\n",
        "\"\"\"\n",
        "\n",
        "analysis = extract_entities(policy_text)\n",
        "print(\"\\nText Analysis:\")\n",
        "for key, value in analysis.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "6OVAIj-1LrIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE 4:\n",
        "\n",
        "Create a function that:\n",
        "1. Takes a political text as input\n",
        "2. Identifies and counts key policy-related terms\n",
        "3. Finds the most frequent phrases (2-3 words)\n",
        "4. Returns a summary of the findings"
      ],
      "metadata": {
        "id": "HC6nB4eQLmO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "W2M7FCK6MEQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINAL PROJECT IDEAS:\n",
        "\n",
        "1. Policy Document Analyzer\n",
        "   - Read a policy document\n",
        "   - Clean and normalize the text\n",
        "   - Extract key points\n",
        "   - Generate a summary\n",
        "   - Count specific policy-related terms\n",
        "\n",
        "2. Speech Comparison Tool\n",
        "   - Read two political speeches\n",
        "   - Compare vocabulary usage\n",
        "   - Analyze sentiment\n",
        "   - Find common themes\n",
        "   - Visualize differences\n",
        "\n",
        "3. Legislative Text Processor\n",
        "   - Extract sections and subsections\n",
        "   - Find definitions\n",
        "   - Track amendments\n",
        "   - Create a searchable index"
      ],
      "metadata": {
        "id": "vkJjWrwDMHJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional Helper Functions\n",
        "\n",
        "def count_word_frequencies(text):\n",
        "    \"\"\"\n",
        "    Count word frequencies in cleaned text\n",
        "    \"\"\"\n",
        "    words = word_tokenize(clean_text(text))\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return Counter(filtered_words)\n",
        "\n",
        "def find_keywords(text, keywords):\n",
        "    \"\"\"\n",
        "    Find instances of specific keywords in text\n",
        "    \"\"\"\n",
        "    cleaned_text = clean_text(text)\n",
        "    found_keywords = {}\n",
        "    for keyword in keywords:\n",
        "        count = len(re.findall(r'\\b' + re.escape(keyword) + r'\\b', cleaned_text))\n",
        "        found_keywords[keyword] = count\n",
        "    return found_keywords\n"
      ],
      "metadata": {
        "id": "Bzfu9hhfMOjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE 5:\n",
        "\n",
        "Final Integration Exercise:\n",
        "\n",
        "Create a complete text analysis pipeline that:\n",
        "1. Reads a political document\n",
        "2. Cleans and normalizes the text\n",
        "3. Extracts key information\n",
        "4. Generates a structured report\n",
        "\n",
        "Use all the concepts we've covered in this module!"
      ],
      "metadata": {
        "id": "beCgZzTnMTMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💡 TIPS & BEST PRACTICES 💡\n",
        "\n",
        "1. File Handling Tips:\n",
        "   - ALWAYS use 'with' statements when working with files (auto-closes files)\n",
        "   - ALWAYS specify encoding (usually 'utf-8') to handle special characters\n",
        "   - For large files, use generators and process line by line\n",
        "   - Keep backup copies of original files before modifying\n",
        "   - Use meaningful file names and organize by project/date\n",
        "\n",
        "2. Error Handling Tips:\n",
        "   - Wrap file operations in try-except blocks\n",
        "   - Check if file exists before operations\n",
        "   - Validate file formats and encodings\n",
        "   - Log errors for debugging\n",
        "\n",
        "3. Text Cleaning Tips:\n",
        "   - Clean text in stages, saving intermediate results\n",
        "   - Create custom cleaning functions for specific needs\n",
        "   - Document all cleaning steps for reproducibility\n",
        "   - Keep original text separate from cleaned versions\n",
        "   - Consider domain-specific cleaning needs\n",
        "\n",
        "4. Performance Tips:\n",
        "   - Use sets for fast lookups\n",
        "   - Compile regex patterns if used multiple times\n",
        "   - Use list comprehensions instead of loops where possible\n",
        "   - Process large files in chunks\n",
        "   - Use appropriate data structures (e.g., Counter for frequencies)\n",
        "\n",
        "5. Political Science-Specific Tips:\n",
        "   - Preserve proper nouns and acronyms\n",
        "   - Handle special cases like bill numbers\n",
        "   - Consider geographical references\n",
        "   - Maintain chronological markers\n",
        "   - Pay attention to political terminology\n",
        "\n",
        "6. Common Pitfalls to Avoid:\n",
        "   - Don't modify original files directly\n",
        "   - Don't assume all files are in English\n",
        "   - Don't remove all numbers (might be important)\n",
        "   - Don't forget to handle edge cases\n",
        "   - Don't clean text more than necessary"
      ],
      "metadata": {
        "id": "ai3F_uyZHi38"
      }
    }
  ]
}