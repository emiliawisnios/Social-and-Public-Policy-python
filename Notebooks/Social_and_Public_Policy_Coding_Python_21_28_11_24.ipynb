{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWzNLx1hhEgVrNSEGAp390",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emiliawisnios/Social-and-Public-Policy-python/blob/main/Notebooks/Social_and_Public_Policy_Coding_Python_21_28_11_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection from the Web\n",
        "\n",
        "Today we will cover web scrapping and API usage.\n",
        "\n",
        "Next time we will focus on OCR."
      ],
      "metadata": {
        "id": "hKXKoj5O21qJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First, we need to install required libraries. Run this cell first:\n"
      ],
      "metadata": {
        "id": "6_KqkcDW3Q86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6CBDlTP3Vr7",
        "outputId": "f24f8396-444a-4de8-df35-0f2b05822567"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QfwNJ7vc2yjF"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HTML Basics\n"
      ],
      "metadata": {
        "id": "RM9g9gb04pfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_html = \"\"\"\n",
        "<html>\n",
        "    <body>\n",
        "        <h1>Congressional Representatives</h1>\n",
        "        <div class=\"representative\">\n",
        "            <h2>Jane Smith</h2>\n",
        "            <p class=\"party\">Democratic Party</p>\n",
        "            <p class=\"state\">California</p>\n",
        "        </div>\n",
        "    </body>\n",
        "</html>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3oc_Z0ib2y74"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's parse this HTML\n",
        "soup = BeautifulSoup(simple_html, 'html.parser')\n",
        "print(\"Finding the title:\")\n",
        "print(soup.find('h1').text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k2WwJVt4xTN",
        "outputId": "626a9d79-2472-4e37-e67a-6a84269c52a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding the title:\n",
            "Congressional Representatives\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Web Scraping Exercise"
      ],
      "metadata": {
        "id": "Nnbi3LmW40eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try scraping a real (but simple) website\n",
        "def get_webpage_content(url):\n",
        "    \"\"\"\n",
        "    Safely fetch webpage content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Add headers to mimic a browser request\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Check for errors\n",
        "        return response.text\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "wILX9bMU43sn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Scrape Wikipedia's List of Current U.S. Senators"
      ],
      "metadata": {
        "id": "UirPyD8S474I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(element):\n",
        "    \"\"\"Helper function to clean text from elements\"\"\"\n",
        "    if element is None:\n",
        "        return \"\"\n",
        "    # Remove hidden spans and unnecessary whitespace\n",
        "    for hidden in element.find_all('span', style='display:none'):\n",
        "        hidden.decompose()\n",
        "    # Remove ForceAgeToShow spans\n",
        "    for age in element.find_all('span', class_='noprint ForceAgeToShow'):\n",
        "        age.decompose()\n",
        "    return element.get_text(strip=True)\n",
        "\n",
        "def get_cell_value(cells, index, default=\"\"):\n",
        "    \"\"\"Safely get cell value at index\"\"\"\n",
        "    try:\n",
        "        return clean_text(cells[index]) if cells[index] else default\n",
        "    except IndexError:\n",
        "        return default\n",
        "\n",
        "def scrape_senators_example():\n",
        "    url = \"https://en.wikipedia.org/wiki/List_of_current_United_States_senators\"\n",
        "    content = get_webpage_content(url)\n",
        "\n",
        "    if content:\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "        senators_data = []\n",
        "\n",
        "        # Find the table with id \"senators\"\n",
        "        table = soup.find('table', {'id': 'senators'})\n",
        "        if table:\n",
        "            # Get all rows except the header row\n",
        "            rows = table.find('tbody').find_all('tr')[1:]  # Skip header row\n",
        "            current_state = None\n",
        "\n",
        "            for row in rows:\n",
        "                cells = row.find_all(['td', 'th'])\n",
        "                if not cells:  # Skip empty rows\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Handle state (might be in rowspan)\n",
        "                    first_cell = cells[0]\n",
        "                    if first_cell.name == 'td' and first_cell.has_attr('rowspan'):\n",
        "                        current_state = clean_text(first_cell)\n",
        "                        state = current_state\n",
        "                    else:\n",
        "                        state = current_state if current_state else clean_text(first_cell)\n",
        "\n",
        "                    # Find the name cell (it's a th)\n",
        "                    name_cell = row.find('th')\n",
        "                    if name_cell:\n",
        "                        name_link = name_cell.find('a')\n",
        "                        if name_link:\n",
        "                            name = clean_text(name_link)\n",
        "\n",
        "                            # Find party cell - it's two cells after the name\n",
        "                            party_index = None\n",
        "                            for i, cell in enumerate(cells):\n",
        "                                if cell.name == 'th' and name_link in cell.find_all('a'):\n",
        "                                    party_index = i + 2\n",
        "                                    break\n",
        "\n",
        "                            if party_index is not None:\n",
        "                                senator_info = {\n",
        "                                    'State': state,\n",
        "                                    'Name': name,\n",
        "                                    'Party': get_cell_value(cells, party_index),\n",
        "                                    'Born': get_cell_value(cells, party_index + 1),\n",
        "                                    'Occupation': get_cell_value(cells, party_index + 2),\n",
        "                                    'Previous_Office': get_cell_value(cells, party_index + 3),\n",
        "                                    'Education': get_cell_value(cells, party_index + 4),\n",
        "                                    'Assumed_Office': get_cell_value(cells, party_index + 5)\n",
        "                                }\n",
        "                                # Validate the data before adding\n",
        "                                if senator_info['Name'] and senator_info['Party']:\n",
        "                                    senators_data.append(senator_info)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing row: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Convert to DataFrame and clean up\n",
        "        df = pd.DataFrame(senators_data)\n",
        "\n",
        "        # Clean up party column - remove any footnotes\n",
        "        df['Party'] = df['Party'].str.replace(r'\\[.*\\]', '', regex=True)\n",
        "\n",
        "        # Remove any rows without essential information\n",
        "        df = df.dropna(subset=['Name', 'Party'])\n",
        "\n",
        "        return df\n",
        "\n",
        "    return None\n",
        "\n",
        "# Example usage with error checking\n",
        "print(\"Fetching current senators...\")\n",
        "try:\n",
        "    senators_df = scrape_senators_example()\n",
        "    if senators_df is not None and not senators_df.empty:\n",
        "        print(\"\\nFirst few senators:\")\n",
        "        print(senators_df[['State', 'Name', 'Party']].head())\n",
        "        print(f\"\\nTotal senators found: {len(senators_df)}\")\n",
        "    else:\n",
        "        print(\"No data was retrieved.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "739pzTdl5HFG",
        "outputId": "d2789ff7-616c-4317-fddf-f57a7006cb15"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching current senators...\n",
            "\n",
            "First few senators:\n",
            "     State              Name        Party\n",
            "0  Alabama  Tommy Tuberville   Republican\n",
            "1  Alabama       Katie Britt   Republican\n",
            "2   Alaska    Lisa Murkowski   Republican\n",
            "3   Alaska      Dan Sullivan   Republican\n",
            "4  Arizona    Kyrsten Sinema  Independent\n",
            "\n",
            "Total senators found: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE 1:\n",
        "Modify the above code to get additional information about senators.\n",
        "Try adding these fields to the senator_info dictionary:\n",
        "- Assumed office date\n",
        "- Born (age)"
      ],
      "metadata": {
        "id": "xYI4LUs--w65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "N4Oerm0K-wRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Web Scraping Ethics\n",
        "\n",
        "IMPORTANT ETHICAL CONSIDERATIONS:\n",
        "\n",
        "1. Always check robots.txt first\n",
        "   - Visit website.com/robots.txt before scraping\n",
        "   - Example: https://en.wikipedia.org/robots.txt\n",
        "\n",
        "2. Be gentle with websites:\n",
        "   - Add delays between requests\n",
        "   - Don't overwhelm servers\n",
        "   \n",
        "3. Check Terms of Service\n",
        "   - Some websites prohibit scraping\n",
        "   - Others have specific APIs you should use instead\n",
        "\n",
        "4. Best Practices:\n",
        "   - Identify your scraper (use proper User-Agent)\n",
        "   - Cache data when possible\n",
        "   - Don't distribute copyrighted content"
      ],
      "metadata": {
        "id": "Be4DstLi-657"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def polite_scraper(urls, delay=1):\n",
        "    \"\"\"\n",
        "    A polite scraper that waits between requests\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for url in urls:\n",
        "        # Get content\n",
        "        content = get_webpage_content(url)\n",
        "        if content:\n",
        "            results.append(content)\n",
        "\n",
        "        # Be polite, wait before next request\n",
        "        time.sleep(delay)\n",
        "    return results"
      ],
      "metadata": {
        "id": "CR704HPS_B_I"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APIs\n",
        "\n",
        "APIs (Application Programming Interfaces) are easier and more reliable\n",
        "than web scraping. Many political data sources have APIs:\n",
        "\n",
        "- Congress.gov API\n",
        "- OpenSecrets API\n",
        "- Federal Election Commission (FEC) API\n",
        "- Data.gov APIs"
      ],
      "metadata": {
        "id": "OrVDjUOA_RgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def get_api_data(url, params=None):\n",
        "    \"\"\"\n",
        "    Generic function to get data from an API\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error accessing API: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example: Using the Open States API (note: you'll need an API key)\n",
        "def get_state_bills_example(state, api_key):\n",
        "    \"\"\"\n",
        "    Get recent bills from a specific state\n",
        "    \"\"\"\n",
        "    url = f\"https://v3.openstates.org/bills\"\n",
        "    params = {\n",
        "        \"jurisdiction\": state,\n",
        "        \"apikey\": api_key\n",
        "    }\n",
        "    return get_api_data(url, params)"
      ],
      "metadata": {
        "id": "9nDlErNw_Xhu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE 2:\n",
        "Register for an API key at data.gov and try to:\n",
        "1. Get data about government agencies\n",
        "2. Convert the response into a pandas DataFrame\n",
        "3. Save the results to a CSV file\n",
        "\n",
        "Template code below:"
      ],
      "metadata": {
        "id": "Fu44BBMS_bf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_government_data(api_key):\n",
        "    base_url = \"https://api.data.gov/...\"  # Students will need to fill this in\n",
        "    params = {\n",
        "        \"api_key\": api_key,\n",
        "        # Add any other parameters\n",
        "    }\n",
        "\n",
        "    # Get the data\n",
        "    data = get_api_data(base_url, params)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    # df = pd.DataFrame(...)\n",
        "\n",
        "    # Save to CSV\n",
        "    # df.to_csv(...)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "fOyuzdi6_i74"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E5RBH7Hr_nAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling API Rate Limits"
      ],
      "metadata": {
        "id": "dI0OXQ18_h4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rate_limited_api_call(url, calls_per_second=1):\n",
        "    \"\"\"\n",
        "    Make API calls while respecting rate limits\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Wait if we're making calls too quickly\n",
        "    elapsed = time.time() - start_time\n",
        "    if elapsed < 1/calls_per_second:\n",
        "        time.sleep(1/calls_per_second - elapsed)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "7u0Q3O09_pF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework\n",
        "\n",
        "Scrap some content from the website of your choice."
      ],
      "metadata": {
        "id": "HFfqO8Wz_wwc"
      }
    }
  ]
}