{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtASayE5UtW+2CSmDbNeMK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emiliawisnios/Social-and-Public-Policy-python/blob/main/Notebooks/Social_and_Public_Policy_Coding_Python_05_12_12_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today's class we will talk about OCR - Optical Character Recognition.\n",
        "\n",
        "\n",
        "Optical Character Recognition (OCR) is the technology that enables computers to extract text from images.\n",
        "Common applications in political science:\n",
        "- Digitizing historical political documents and archives\n",
        "- Processing campaign materials and political advertisements\n",
        "- Analyzing scanned policy documents\n",
        "- Converting protest signs and banners to text\n",
        "- Processing voting ballots and election materials"
      ],
      "metadata": {
        "id": "1yJUmGNDwttI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tesseract-ocr"
      ],
      "metadata": {
        "id": "IUIKTAHY26gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract opencv-python pillow numpy matplotlib --q"
      ],
      "metadata": {
        "id": "sNJq9WVpzyCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNuJixZrwji2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_basic_ocr(image_path):\n",
        "    \"\"\"\n",
        "    Performs basic OCR on an image file.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text from the image\n",
        "    \"\"\"\n",
        "    # Read the image\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Extract text\n",
        "    text = pytesseract.image_to_string(image)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "lEZxs6fOz9MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "\n",
        "file_url = 'https://raw.githubusercontent.com/emiliawisnios/Social-and-Public-Policy-python/refs/heads/main/Documents/nr057532-1.png'\n",
        "urlretrieve(file_url, \"image.jpg\")"
      ],
      "metadata": {
        "id": "7L6m_6NJ2Had"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = perform_basic_ocr('/content/image.jpg')\n",
        "print(\"Extracted text:\")\n",
        "print(sample_text)"
      ],
      "metadata": {
        "id": "u51QXfIt2Kv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_detailed_ocr_info(image_path):\n",
        "    \"\"\"\n",
        "    Gets detailed OCR information including confidence scores and bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing detailed OCR information\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Get detailed OCR data\n",
        "    ocr_data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
        "\n",
        "    # Convert to DataFrame for easier analysis\n",
        "    df = pd.DataFrame(ocr_data)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "fwNFG1fg3uGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = get_detailed_ocr_info('/content/image.jpg')\n",
        "print(\"\\nAverage confidence score:\", df['conf'].mean())\n",
        "print(\"\\nWords with confidence > 90%:\")\n",
        "print(df[df['conf'] > 90][['text', 'conf']])"
      ],
      "metadata": {
        "id": "Vhs9aYV33yN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "i1RK4_IQ3_Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task\n",
        "\n",
        "Get yor own document from the web and do OCR."
      ],
      "metadata": {
        "id": "0MXoGIql4aKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### YOUR CODE GOES HERE #####"
      ],
      "metadata": {
        "id": "p1bmMR2v4g0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Preprocessing Functions"
      ],
      "metadata": {
        "id": "vcNFFHyq4mBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    \"\"\"\n",
        "    Applies various preprocessing techniques to improve OCR accuracy.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Preprocessed image\n",
        "    \"\"\"\n",
        "    # Read image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply thresholding to preprocess the image\n",
        "    gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
        "\n",
        "    # Apply dilation to connect text components\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
        "    gray = cv2.dilate(gray, kernel, iterations=1)\n",
        "\n",
        "    # Apply noise reduction\n",
        "    gray = cv2.medianBlur(gray, 3)\n",
        "\n",
        "    return gray\n"
      ],
      "metadata": {
        "id": "wHKIOYHr4qzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image_comparison(original_path, preprocessed_image):\n",
        "    \"\"\"\n",
        "    Displays original and preprocessed images side by side.\n",
        "\n",
        "    Args:\n",
        "        original_path (str): Path to original image\n",
        "        preprocessed_image (numpy.ndarray): Preprocessed image\n",
        "    \"\"\"\n",
        "    original = cv2.imread(original_path)\n",
        "    original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(original_rgb)\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(preprocessed_image, cmap='gray')\n",
        "    plt.title('Preprocessed Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-ysTDPCO4vQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_advanced_ocr(image_path):\n",
        "    \"\"\"\n",
        "    Performs OCR with preprocessing steps for better accuracy.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "\n",
        "    Returns:\n",
        "        tuple: (preprocessed_text, original_text, confidence_comparison)\n",
        "    \"\"\"\n",
        "    # Original OCR\n",
        "    original_text = perform_basic_ocr(image_path)\n",
        "\n",
        "    # Preprocess and perform OCR\n",
        "    preprocessed_image = preprocess_image(image_path)\n",
        "    preprocessed_text = pytesseract.image_to_string(preprocessed_image)\n",
        "\n",
        "    # Compare confidence scores\n",
        "    original_conf = pytesseract.image_to_data(Image.open(image_path),\n",
        "                                            output_type=pytesseract.Output.DICT)\n",
        "    preprocessed_conf = pytesseract.image_to_data(preprocessed_image,\n",
        "                                                output_type=pytesseract.Output.DICT)\n",
        "\n",
        "    conf_comparison = {\n",
        "        'original_mean_conf': np.mean([conf for conf in original_conf['conf'] if conf != -1]),\n",
        "        'preprocessed_mean_conf': np.mean([conf for conf in preprocessed_conf['conf'] if conf != -1])\n",
        "    }\n",
        "\n",
        "    return preprocessed_text, original_text, conf_comparison"
      ],
      "metadata": {
        "id": "3QiCewTS41MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def improve_image_quality(image_path):\n",
        "    \"\"\"\n",
        "    Applies additional preprocessing techniques for challenging images.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Enhanced image\n",
        "    \"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Increase contrast\n",
        "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "    cl = clahe.apply(l)\n",
        "    enhanced = cv2.merge((cl,a,b))\n",
        "    enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    # Denoise\n",
        "    enhanced = cv2.fastNlMeansDenoisingColored(enhanced, None, 10, 10, 7, 21)\n",
        "\n",
        "    return enhanced\n",
        "\n",
        "\n",
        "# 5. Evaluation Functions\n",
        "def evaluate_ocr_quality(predicted_text, ground_truth):\n",
        "    \"\"\"\n",
        "    Evaluates OCR quality using basic metrics.\n",
        "\n",
        "    Args:\n",
        "        predicted_text (str): OCR output text\n",
        "        ground_truth (str): Correct text\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    from difflib import SequenceMatcher\n",
        "\n",
        "    # Calculate similarity ratio\n",
        "    similarity = SequenceMatcher(None, predicted_text, ground_truth).ratio()\n",
        "\n",
        "    # Word accuracy (simple implementation)\n",
        "    pred_words = set(predicted_text.lower().split())\n",
        "    true_words = set(ground_truth.lower().split())\n",
        "    word_accuracy = len(pred_words.intersection(true_words)) / len(true_words)\n",
        "\n",
        "    return {\n",
        "        'similarity_ratio': similarity,\n",
        "        'word_accuracy': word_accuracy\n",
        "    }"
      ],
      "metadata": {
        "id": "qr0cRuf343nC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}