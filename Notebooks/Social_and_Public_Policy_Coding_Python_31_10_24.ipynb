{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw2X+AzMFvHdRQBY30lmcq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emiliawisnios/Social-and-Public-Policy-python/blob/main/Notebooks/Social_and_Public_Policy_Coding_Python_31_10_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today's class we will focus on fundamentals of text processing and NLP.\n",
        "\n",
        "Next time we will work on data scraping."
      ],
      "metadata": {
        "id": "WmtPQ-mobqar"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MalmCGIaFzP"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speeches = {\n",
        "    'kennedy': \"\"\"Ask not what your country can do for you â€“ ask what you can do for your country.\n",
        "                 Let both sides seek to invoke the wonders of science instead of its terrors.\"\"\",\n",
        "    'mlk': \"\"\"I have a dream that one day this nation will rise up and live out the true meaning of its creed:\n",
        "              We hold these truths to be self-evident, that all men are created equal.\"\"\"\n",
        "}"
      ],
      "metadata": {
        "id": "gqlORgJCcYLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing"
      ],
      "metadata": {
        "id": "GFD9P_Weccpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_steps(text, step_name):\n",
        "    \"\"\"Helper function to display results of each preprocessing step\"\"\"\n",
        "    print(f\"\\n{step_name}:\")\n",
        "    print(text[:150], \"...\" if len(text) > 150 else \"\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "B9JRIwlZcjGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "Tokenization is the process of breaking text into individual words or sentences."
      ],
      "metadata": {
        "id": "IJ87LK_xcikC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokenization\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Sentence Tokenization\n",
        "def sentence_tokenize(text):\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "# Example with Kennedy's speech\n",
        "kennedy_speech = speeches['kennedy']\n",
        "print(\"\\nOriginal text:\")\n",
        "print(kennedy_speech)\n",
        "\n",
        "word_tokens = tokenize_text(kennedy_speech)\n",
        "print(\"\\nWord tokens:\")\n",
        "print(word_tokens)\n",
        "\n",
        "sent_tokens = sentence_tokenize(kennedy_speech)\n",
        "print(\"\\nSentence tokens:\")\n",
        "print(sent_tokens)"
      ],
      "metadata": {
        "id": "RnlVqmMYc7rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming reduces words to their root/base form, sometimes producing non-real words."
      ],
      "metadata": {
        "id": "zKFbh3rcc_79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "def stem_text(tokens):\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "stemmed_words = stem_text(word_tokens)\n",
        "print(\"\\nOriginal vs Stemmed words:\")\n",
        "for orig, stemmed in zip(word_tokens[:10], stemmed_words[:10]):\n",
        "    print(f\"{orig:15} -> {stemmed}\")"
      ],
      "metadata": {
        "id": "o3vaO2wNdERD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization reduces words to their dictionary base form, always producing real words."
      ],
      "metadata": {
        "id": "MbD5dO0YdH8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_text(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "lemmatized_words = lemmatize_text(word_tokens)\n",
        "print(\"\\nOriginal vs Lemmatized words:\")\n",
        "for orig, lemma in zip(word_tokens[:10], lemmatized_words[:10]):\n",
        "    print(f\"{orig:15} -> {lemma}\")"
      ],
      "metadata": {
        "id": "Y1uCYr3DdMeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop word removal\n",
        "\n",
        "Removing common words that don't carry significant meaning."
      ],
      "metadata": {
        "id": "WOJ_4sJzdQ64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "filtered_words = remove_stopwords(word_tokens)\n",
        "print(\"\\nOriginal tokens:\", word_tokens)\n",
        "print(\"\\nAfter stopword removal:\", filtered_words)"
      ],
      "metadata": {
        "id": "-cSy4oGKdVSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extraction"
      ],
      "metadata": {
        "id": "J2jYxrzidZPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BAG OF WORDS\n",
        "\n",
        "Converting text into numerical features based on word frequency."
      ],
      "metadata": {
        "id": "H5YwlrTEdcRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Create corpus from both speeches\n",
        "corpus = list(speeches.values())\n",
        "\n",
        "# Generate BoW representation\n",
        "bow_matrix = count_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "bow_df = pd.DataFrame(\n",
        "    bow_matrix.toarray(),\n",
        "    columns=count_vectorizer.get_feature_names_out(),\n",
        "    index=['Kennedy Speech', 'MLK Speech']\n",
        ")\n",
        "\n",
        "print(\"\\nBag of Words representation:\")\n",
        "print(bow_df)"
      ],
      "metadata": {
        "id": "2LHZg-oSdg0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "Weighting words based on their frequency and importance across documents."
      ],
      "metadata": {
        "id": "25oSMpGMdkdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate TF-IDF representation\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
        "    index=['Kennedy Speech', 'MLK Speech']\n",
        ")\n",
        "\n",
        "print(\"\\nTF-IDF representation:\")\n",
        "print(tfidf_df)"
      ],
      "metadata": {
        "id": "nQM0Xpsldo2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word embeddings"
      ],
      "metadata": {
        "id": "11-uvuHvdrwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating dense vector representations of words that capture semantic relationships."
      ],
      "metadata": {
        "id": "-Hw6jjYJdyx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for Word2Vec\n",
        "tokenized_corpus = [word_tokenize(speech.lower()) for speech in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1)\n",
        "\n",
        "# Function to find similar words\n",
        "def find_similar_words(word, model):\n",
        "    try:\n",
        "        similar_words = model.wv.most_similar(word)\n",
        "        print(f\"\\nWords most similar to '{word}':\")\n",
        "        for w, score in similar_words:\n",
        "            print(f\"{w}: {score:.4f}\")\n",
        "    except KeyError:\n",
        "        print(f\"'{word}' not in vocabulary\")\n",
        "\n",
        "# Example similar words\n",
        "print(\"\\nExploring word similarities:\")\n",
        "find_similar_words(\"country\", word2vec_model)"
      ],
      "metadata": {
        "id": "AYKJI69Dd4NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework"
      ],
      "metadata": {
        "id": "Aj47-MJ1d-6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Text Analysis Exercise:\n",
        "   - Choose a political speech or document of your interest\n",
        "   - Apply the preprocessing steps learned (tokenization, stemming, lemmatization)\n",
        "   - Compare the results of different preprocessing techniques\n",
        "   \n",
        "2. Comparative Analysis Exercise:\n",
        "   - Select two political texts from different periods or contexts\n",
        "   - Create and compare their BoW and TF-IDF representations\n",
        "   - What insights can you draw about their language and themes?\n",
        "   \n",
        "3. Word Embedding Exercise:\n",
        "   - Using the Word2Vec model, explore relationships between political concepts\n",
        "   - Find similar words for terms like 'democracy', 'freedom', 'justice'\n",
        "   - What patterns do you observe in the semantic relationships?"
      ],
      "metadata": {
        "id": "9k33fBmPeA6s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LKBtJsL7cps6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}